{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification with Convolutional Neural Networks\n",
    "Now we will do image classification on the mnist data that we have been looking at in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to work out what number an image is. There are 10 possible classes so rather than having binary output labels, we will need vectors of size 10. You have seen the other parameters before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the mnist dataset directly from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 60000 images to train on, and 60000 more to test on. For now, we will work on a small subset of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:5000, :,:]\n",
    "y_train = y_train[:5000]\n",
    "x_test = x_test[:5000, :,:]\n",
    "y_test = y_test[:5000]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution neural networks are usually done on colour image data, this is usually `height x width x 3` where we have the three colour channels: red, green and blue. keras convolutional layers are built for this, so we need to add an extra channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 # This converts the pixel values from between 0 and 255 to between 0 and 1\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to set up y_train and y_test to have a vector of size 10 for each input sample.\n",
    "\n",
    "y-train is the training label. For an image of a handwritten 0, it should be `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]`; for a 1, it should be `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]` etc.\n",
    "\n",
    "y_test is the output vector of confidence scores that the model will return. For an image of a one, we might expect something like `[0, 0.9, 0, 0, 0, 0, 0, 0.1, 0, 0]`. The model is 90% sure it is a 1, and 10% sure it is a 7 (because some people write a 7 similarly to a 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "Again we need a sequential model. Now however, instead of `Dense` layers, we will use `Conv2d` layers. These are very similar to the Dense layers we saw previously, but have an added variable `kernel_size`. Everything else in these layers you have seen before. Note that because we switch from binary classification to multi-class classification, the final activation function becomes a softmax.\n",
    "\n",
    "Recall with convolutional layers that we take a feature and scan across the image, looking for parts of the image that look similar to the feature. The mathematical term for this feature is a `kernel`. A kernel size of `(3,3)` will give a 3x3 matrix on our data. So a 3x3 matrix will scan across the whole image.\n",
    "\n",
    "Note that when we have colour images, a kernel size of `(3, 3)` is a 3x3x3 tensor, as we have 3 colour channels. You don't need to worry about this, keras will automatically match up the kernel size with the number of channels.\n",
    "#### Max Pooling\n",
    "We also have a Max Pooling layer, to help reduce the image size. Max pooling, with a pool size of `(2,2)`, splits the image up into 2x2 chunks, finds the maximum number of those chunks, and deletes the other values. This will have the dimensions of the images you are working with\n",
    "You can also do average pooling, but it is a little less efficient.\n",
    "\n",
    "\n",
    "\n",
    "#### Visualising the model\n",
    "Use model.summary() to see all the input dimensions at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten()) # This line is to convert from matrices to vectors\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax')) # we are working with vectors now, so we use a Dense layer instead of Conv2d\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We fit the model in the same way as in the previous notebook (`verbose = 1` hasn't got anything to do with fitting the model, just how results are presented). \n",
    "\n",
    "The biggest difference will be how much slower the network is, even with our small subset of the data. We will learn how to use GPUs to speed this up in the next practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Even though convolutional neural networks are more complicated conceptually than regular neural networks, they are not too difficult to code.\n",
    "\n",
    "For all of these tasks, the most laborious part has been setting up the data in such a way that it can be processed. Once that has been managed, keras will do all the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "As before, do what you can to increase the accuracy of the model. Play around with more layers, more channels, a different batch size. Try to get as close to 100 as you can. Then try using more data, depending on how powerful the machine you have is. \n",
    "\n",
    "### Exercise 2\n",
    "Convert the numpy data to 784x1 vectors as we did in the lecture. Compose a neural network (like we did in the previous notebook) and try to get as close to you test accuracy with convolutional neural networks as you can."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
