{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of Diabetes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D9lFuPIj8jp"
      },
      "source": [
        "## Diabetes Diagnosis\n",
        "We are going to see how a neural network could be used to diagnose diabetes. When you go through this notebook, pay attention to how little human intervention is needed, the neural network does all the work for us!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe5eKZ5Gj8ju"
      },
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xABi92fvj8jv"
      },
      "source": [
        "### Exercise 1: Load the data\n",
        "\n",
        "Before we load the data, we must upload it to colab. On the left you should see four symbols, the bottom one refers to files.\n",
        "\n",
        "Read the line below and work out where the correct place to upload the data is. Hint: to create a new folder, right click under `sample data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBLaWPRJj8jv"
      },
      "source": [
        "dataframe = pd.read_csv(\"Data/diabetes.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nefJzxkbj8jv"
      },
      "source": [
        "Now the data is loaded, lets have a look at it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "4kvYflxuj8jw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "844349c5-fbe3-4c24-e526-50a0f87a1055"
      },
      "source": [
        "dataframe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UxVkYvMj8jw"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "You may notice that there are a lot of '0' values in this data. With a touch of common sense, we can work out which are legitimate zeros and which are missing values. It's perfectly normal to have had 0 pregnancies, but I hope to never meet anyone with a skin thickness of 0!\n",
        "\n",
        "Use the missing value imputation methods you have seen previously to fill in missing values in this data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AZ9k6P0nAPH"
      },
      "source": [
        "### Solution\r\n",
        "First lets take a look at columns individually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI7wzuJVnZ0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2004f1ce-8506-4985-c7a3-4fffced3489e"
      },
      "source": [
        "dataframe['Insulin']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        0\n",
              "3       94\n",
              "4      168\n",
              "      ... \n",
              "763    180\n",
              "764      0\n",
              "765    112\n",
              "766      0\n",
              "767      0\n",
              "Name: Insulin, Length: 768, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcX-Ln6nnhW-"
      },
      "source": [
        "Play around with stuff to see what happens. We are going to need the `.where()` function or the `replace` function. `where` is a less efficient way of doing things so I will use `replace`, but I first show how to do a simple replacement with `where`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxn7S24nnzW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c1848e7-0a88-4d41-8bf8-fcba080e0ca0"
      },
      "source": [
        "dataframe['Insulin'].where(dataframe['Insulin']==0) # Looks promising"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0.0\n",
              "1      0.0\n",
              "2      0.0\n",
              "3      NaN\n",
              "4      NaN\n",
              "      ... \n",
              "763    NaN\n",
              "764    0.0\n",
              "765    NaN\n",
              "766    0.0\n",
              "767    0.0\n",
              "Name: Insulin, Length: 768, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjfcIFquoGsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643767e0-a93e-4ead-9fc1-b106d96847c3"
      },
      "source": [
        "dataframe['Insulin'].where(dataframe['Insulin']==0, -1) #This seems to replace the wrong values with -1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3     -1\n",
              "4     -1\n",
              "      ..\n",
              "763   -1\n",
              "764    0\n",
              "765   -1\n",
              "766    0\n",
              "767    0\n",
              "Name: Insulin, Length: 768, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY3pzs4WpdpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a42fc75-d220-4917-bda0-03fa8fe40fb8"
      },
      "source": [
        "dataframe['Insulin'].where(dataframe['Insulin']!=0, -1) #This seems to be what we want"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       -1\n",
              "1       -1\n",
              "2       -1\n",
              "3       94\n",
              "4      168\n",
              "      ... \n",
              "763    180\n",
              "764     -1\n",
              "765    112\n",
              "766     -1\n",
              "767     -1\n",
              "Name: Insulin, Length: 768, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrvBlE4doYQC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "dab50132-cb2f-4ea5-cd79-6619bc1c87cb"
      },
      "source": [
        "dataframe['Insulin'] = dataframe['Insulin'].where(dataframe['Insulin']!=0, -1) #This works but it is very messy!\r\n",
        "dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>-1</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>-1</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>-1</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>-1</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  ...  Age  Outcome\n",
              "0              6      148  ...   50        1\n",
              "1              1       85  ...   31        0\n",
              "2              8      183  ...   32        1\n",
              "3              1       89  ...   21        0\n",
              "4              0      137  ...   33        1\n",
              "..           ...      ...  ...  ...      ...\n",
              "763           10      101  ...   63        0\n",
              "764            2      122  ...   27        0\n",
              "765            5      121  ...   30        0\n",
              "766            1      126  ...   47        1\n",
              "767            1       93  ...   23        0\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeurObWAqOZO"
      },
      "source": [
        "dataframe = pd.read_csv(\"Data/diabetes.csv\") #We altered the original dataframe above so lets reload it. Lets try to do the same thing with .replace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r6_dOIZqgDE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "da774d63-e796-49d4-c1f0-dd90248bec4c"
      },
      "source": [
        "dataframe_replace = dataframe.replace(0,-1) #This is much easier, but remember we need to select certain columns\r\n",
        "dataframe_replace # Use dataframe_replace so we do not overwrite the original dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>-1</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>-1</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>-1</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>-1</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  ...  Age  Outcome\n",
              "0              6      148  ...   50        1\n",
              "1              1       85  ...   31       -1\n",
              "2              8      183  ...   32        1\n",
              "3              1       89  ...   21       -1\n",
              "4             -1      137  ...   33        1\n",
              "..           ...      ...  ...  ...      ...\n",
              "763           10      101  ...   63       -1\n",
              "764            2      122  ...   27       -1\n",
              "765            5      121  ...   30       -1\n",
              "766            1      126  ...   47        1\n",
              "767            1       93  ...   23       -1\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sLzmgChj8jw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "68237f9c-42a9-44a5-e041-922c129b53ce"
      },
      "source": [
        "columns = ['Glucose', 'BloodPressure','SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction'] #Select columns where zero shouldn't appear\r\n",
        "dataframe_replace = dataframe.copy() #copy dataframe and update the columns we need individually in the for-loop\r\n",
        "                                     # we need .copy() here otherwise we will overwrite the original dataframe\r\n",
        "\r\n",
        "# Simple Missing Value Imputation\r\n",
        "dataframe_replace[columns] = dataframe[columns].replace(0, -1)\r\n",
        "dataframe_replace #this seems to have worked"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>-1</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>-1</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>-1</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>-1</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  ...  Age  Outcome\n",
              "0              6      148  ...   50        1\n",
              "1              1       85  ...   31        0\n",
              "2              8      183  ...   32        1\n",
              "3              1       89  ...   21        0\n",
              "4              0      137  ...   33        1\n",
              "..           ...      ...  ...  ...      ...\n",
              "763           10      101  ...   63        0\n",
              "764            2      122  ...   27        0\n",
              "765            5      121  ...   30        0\n",
              "766            1      126  ...   47        1\n",
              "767            1       93  ...   23        0\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzfDc6tulSpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "724b7d19-225b-4fa2-9a00-47a1cf541f76"
      },
      "source": [
        "# Now lets try more complicated types of missing value imputation\r\n",
        "dataframe['Insulin'].where(dataframe['Insulin']!=0).mean() #Use the .where() function to get the mean of all non-zero values in a column\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155.5482233502538"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXzgx272spne",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "230f2541-c521-4b4e-87f3-839a9c16edeb"
      },
      "source": [
        "#That seems to work so lets try and replace all columns with this value\r\n",
        "dataframe_mean = dataframe.copy() \r\n",
        "for column in columns:\r\n",
        "  column_mean = dataframe[column].where(dataframe[column]!=0).mean() #calculate the mean of the column\r\n",
        "  dataframe_mean[column] = dataframe[column].replace(0, column_mean) #replace 0s with the column mean\r\n",
        "\r\n",
        "dataframe_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.00000</td>\n",
              "      <td>155.548223</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.00000</td>\n",
              "      <td>155.548223</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>29.15342</td>\n",
              "      <td>155.548223</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.00000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.00000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>48.00000</td>\n",
              "      <td>180.000000</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>27.00000</td>\n",
              "      <td>155.548223</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>23.00000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>29.15342</td>\n",
              "      <td>155.548223</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>31.00000</td>\n",
              "      <td>155.548223</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  ...  Age  Outcome\n",
              "0              6    148.0  ...   50        1\n",
              "1              1     85.0  ...   31        0\n",
              "2              8    183.0  ...   32        1\n",
              "3              1     89.0  ...   21        0\n",
              "4              0    137.0  ...   33        1\n",
              "..           ...      ...  ...  ...      ...\n",
              "763           10    101.0  ...   63        0\n",
              "764            2    122.0  ...   27        0\n",
              "765            5    121.0  ...   30        0\n",
              "766            1    126.0  ...   47        1\n",
              "767            1     93.0  ...   23        0\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt_ihHNCuQBO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "5cbb64d2-9414-4b6a-a12d-b1a8d5e1e8ca"
      },
      "source": [
        "#Now lets do the same with a random number between minimum and maximum\r\n",
        "dataframe_rand = dataframe.copy()\r\n",
        "\r\n",
        "for column in columns:\r\n",
        "  column_max = dataframe[column].max()\r\n",
        "  column_min = dataframe[column].where(dataframe[column]!=0).min() #we need the 'where' here otherwise we will get 0 as the minimum\r\n",
        "  rand = np.random.uniform(column_min, column_max) #random number from the uniform distribution between min and max (you can try from the normal distribution as well)\r\n",
        "  dataframe_rand[column] = dataframe[column].replace(0, rand)\r\n",
        "\r\n",
        "dataframe_rand #This assigns one random value per column, how can you assign a different value for every new zero instance?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>17.717941</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>17.717941</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>47.469012</td>\n",
              "      <td>17.717941</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>180.000000</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>17.717941</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>47.469012</td>\n",
              "      <td>17.717941</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>17.717941</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  ...  Age  Outcome\n",
              "0              6    148.0  ...   50        1\n",
              "1              1     85.0  ...   31        0\n",
              "2              8    183.0  ...   32        1\n",
              "3              1     89.0  ...   21        0\n",
              "4              0    137.0  ...   33        1\n",
              "..           ...      ...  ...  ...      ...\n",
              "763           10    101.0  ...   63        0\n",
              "764            2    122.0  ...   27        0\n",
              "765            5    121.0  ...   30        0\n",
              "766            1    126.0  ...   47        1\n",
              "767            1     93.0  ...   23        0\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWzRUJ0ryQHx"
      },
      "source": [
        "#End of Solution\r\n",
        "You can try loading the different data into the models below and see what seems to work best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_I8wk8Yj8jw"
      },
      "source": [
        "We now need to remove supervision labels from the features that we want to learn from. In this case the outcome column contains the supervision labels, which tells us if a person has diabetes or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX7MMb29j8jx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "4b6d428c-8d58-4a97-9684-9f59f061952e"
      },
      "source": [
        "df_label = dataframe['Outcome']\n",
        "df_features = dataframe.drop('Outcome', 1)\n",
        "print(df_label.head())\n",
        "df_features.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1    0\n",
            "2    1\n",
            "3    0\n",
            "4    1\n",
            "Name: Outcome, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...   BMI  DiabetesPedigreeFunction  Age\n",
              "0            6      148             72  ...  33.6                     0.627   50\n",
              "1            1       85             66  ...  26.6                     0.351   31\n",
              "2            8      183             64  ...  23.3                     0.672   32\n",
              "3            1       89             66  ...  28.1                     0.167   21\n",
              "4            0      137             40  ...  43.1                     2.288   33\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkxsWRvgj8jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d13ce686-3a14-4767-c2f6-ba8dad9c12c4"
      },
      "source": [
        "data = np.array(df_features)\n",
        "label = np.array(df_label)\n",
        "print(data.shape, label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 8) (768,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87jtOzF3j8jx"
      },
      "source": [
        "#### Split the data into train and test portions\n",
        "We need to split the data into training and testing, we will use the function from sklearn to split our samples into 80% train samples and 20% test samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1jFVajyj8jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53462a1-4379-41df-f938-71463290a958"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
        "x_train.shape\n",
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(154, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkhLKsGQj8jy"
      },
      "source": [
        "#### Build the model\n",
        "We will now build the neural network.\n",
        "\n",
        "To do this, we call `model = Sequential()` (remember we imported Sequential in the first cell of this notebook). It is called sequential because neural networks are a sequence: layer 1 then layer 2 then layer 3 etc. So the model is called `model` and it is sequential."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mkZdb8ij8jy"
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWn43NIJj8jy"
      },
      "source": [
        "#### Add layers\n",
        "Next we need to add layers. In keras, a fully connected layer (like you saw in the lecture) is called a Dense layer. It works as follows:\n",
        "    `model.add(Dense(number of neurons, input dimension (optional), activation function))`\n",
        "    \n",
        "The *input dimension* only needs to included for the first layer. For the following layers, keras will automatically include the input dimension as the number of neurons from the previous layer.\n",
        "\n",
        "The *number of neurons* is the number of features we consider at each layer. \n",
        "Note the last layer has only one neuron. This is because our labels are one dimensional. When we have $n>2$ output classes, we'll need $n$ outputs.\n",
        "\n",
        "The *activation* is a non-linear function that is applied at each layer. We will discuss this in the next lecture. In the final layer, this function is the loss function. Again, we will discuss loss functions in detail on Thursday."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P19Yl_wrj8jy"
      },
      "source": [
        "model.add(Dense(50, input_dim=8, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq46I9Q3j8jz"
      },
      "source": [
        "Lastly, to compile the model, we use `model.compile`. This requires three things, the loss, the optimiser, and the metric to optimise on:\n",
        "*Loss*: When we do classification, we perform cross-entropy. In this case, we only have two classes, so we need `binary_crossentropy`\n",
        "*Optimiser*: The optimiser is what we use to update the weights in the network. Traditionally we used Stochastic Gradient Descent (SGD), but a few years ago 'Adam' was proposed and usually outperforms sgd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdK3VjNkj8jz"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH3Uf3D9j8jz"
      },
      "source": [
        "#### Training (and testing) the model\n",
        "\n",
        "To train the model, we use model.fit() which takes on several variables\n",
        "*Training data* this consists of the training features and training labels, in our case `x_train` and `y_train`\n",
        "*Epochs* How many times we will pass through all of the data\n",
        "*Batch size* How many samples we will consider at any time\n",
        "*Validation_data* Test the performance on unseen data. We will use the test data for this. You might also want to split 10% of your training data off to use for validation, so that your test data remains completely unseen until after training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPK_HRgvj8jz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef104e7-5031-4fdb-b9ca-cb2b6ca3084b"
      },
      "source": [
        "model.fit(x_train,y_train, epochs=1000, batch_size=10, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "62/62 [==============================] - 1s 5ms/step - loss: 1.9850 - accuracy: 0.5691 - val_loss: 0.9267 - val_accuracy: 0.6169\n",
            "Epoch 2/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7613 - accuracy: 0.6469 - val_loss: 0.8197 - val_accuracy: 0.6688\n",
            "Epoch 3/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8337 - accuracy: 0.6024 - val_loss: 0.7527 - val_accuracy: 0.6688\n",
            "Epoch 4/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6066 - accuracy: 0.6907 - val_loss: 0.8627 - val_accuracy: 0.6558\n",
            "Epoch 5/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6285 - accuracy: 0.7111 - val_loss: 0.8910 - val_accuracy: 0.6753\n",
            "Epoch 6/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 0.6983 - val_loss: 0.7935 - val_accuracy: 0.6558\n",
            "Epoch 7/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.6602 - val_loss: 0.6787 - val_accuracy: 0.6948\n",
            "Epoch 8/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7154 - val_loss: 1.1054 - val_accuracy: 0.5714\n",
            "Epoch 9/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7599 - accuracy: 0.6422 - val_loss: 0.6969 - val_accuracy: 0.6429\n",
            "Epoch 10/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.6529 - val_loss: 0.8440 - val_accuracy: 0.7013\n",
            "Epoch 11/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5807 - accuracy: 0.6924 - val_loss: 0.7449 - val_accuracy: 0.5714\n",
            "Epoch 12/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6519 - accuracy: 0.6709 - val_loss: 0.6238 - val_accuracy: 0.7013\n",
            "Epoch 13/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6175 - accuracy: 0.7049 - val_loss: 0.8238 - val_accuracy: 0.6883\n",
            "Epoch 14/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.7091 - val_loss: 0.6382 - val_accuracy: 0.7208\n",
            "Epoch 15/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.6964 - val_loss: 0.6622 - val_accuracy: 0.6623\n",
            "Epoch 16/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7532 - val_loss: 0.6524 - val_accuracy: 0.6558\n",
            "Epoch 17/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5784 - accuracy: 0.7298 - val_loss: 0.6858 - val_accuracy: 0.6883\n",
            "Epoch 18/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5379 - accuracy: 0.7394 - val_loss: 0.6388 - val_accuracy: 0.6753\n",
            "Epoch 19/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5528 - accuracy: 0.7138 - val_loss: 0.6763 - val_accuracy: 0.6948\n",
            "Epoch 20/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5986 - accuracy: 0.7276 - val_loss: 0.6782 - val_accuracy: 0.7013\n",
            "Epoch 21/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7745 - val_loss: 0.7334 - val_accuracy: 0.6623\n",
            "Epoch 22/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6454 - accuracy: 0.7128 - val_loss: 0.6561 - val_accuracy: 0.7013\n",
            "Epoch 23/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.7078 - val_loss: 0.6790 - val_accuracy: 0.6818\n",
            "Epoch 24/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7214 - val_loss: 0.6897 - val_accuracy: 0.7143\n",
            "Epoch 25/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5413 - accuracy: 0.7225 - val_loss: 0.6790 - val_accuracy: 0.6818\n",
            "Epoch 26/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5225 - accuracy: 0.7351 - val_loss: 0.6801 - val_accuracy: 0.7013\n",
            "Epoch 27/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.7719 - val_loss: 0.6633 - val_accuracy: 0.6753\n",
            "Epoch 28/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7813 - val_loss: 0.6925 - val_accuracy: 0.5974\n",
            "Epoch 29/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.7196 - val_loss: 0.6561 - val_accuracy: 0.7143\n",
            "Epoch 30/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5545 - accuracy: 0.7268 - val_loss: 0.6766 - val_accuracy: 0.6623\n",
            "Epoch 31/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4925 - accuracy: 0.7468 - val_loss: 0.6915 - val_accuracy: 0.6364\n",
            "Epoch 32/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7531 - val_loss: 0.6229 - val_accuracy: 0.6883\n",
            "Epoch 33/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4466 - accuracy: 0.7685 - val_loss: 0.7073 - val_accuracy: 0.7208\n",
            "Epoch 34/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4799 - accuracy: 0.7606 - val_loss: 0.7001 - val_accuracy: 0.6818\n",
            "Epoch 35/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7430 - val_loss: 0.6512 - val_accuracy: 0.6883\n",
            "Epoch 36/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7694 - val_loss: 0.6855 - val_accuracy: 0.6883\n",
            "Epoch 37/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7732 - val_loss: 0.6764 - val_accuracy: 0.6948\n",
            "Epoch 38/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4788 - accuracy: 0.7788 - val_loss: 0.6736 - val_accuracy: 0.7078\n",
            "Epoch 39/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4817 - accuracy: 0.7886 - val_loss: 0.6585 - val_accuracy: 0.6494\n",
            "Epoch 40/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7794 - val_loss: 0.6217 - val_accuracy: 0.6948\n",
            "Epoch 41/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.7663 - val_loss: 0.6301 - val_accuracy: 0.6818\n",
            "Epoch 42/1000\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.7767 - val_loss: 0.6610 - val_accuracy: 0.6883\n",
            "Epoch 43/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7706 - val_loss: 0.6659 - val_accuracy: 0.7078\n",
            "Epoch 44/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4678 - accuracy: 0.7662 - val_loss: 0.7699 - val_accuracy: 0.6688\n",
            "Epoch 45/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5315 - accuracy: 0.7736 - val_loss: 0.6536 - val_accuracy: 0.6753\n",
            "Epoch 46/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.7646 - val_loss: 0.6807 - val_accuracy: 0.6688\n",
            "Epoch 47/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4630 - accuracy: 0.7644 - val_loss: 0.6538 - val_accuracy: 0.6883\n",
            "Epoch 48/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7484 - val_loss: 0.7001 - val_accuracy: 0.6753\n",
            "Epoch 49/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.7814 - val_loss: 0.6532 - val_accuracy: 0.7208\n",
            "Epoch 50/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7495 - val_loss: 0.6855 - val_accuracy: 0.6818\n",
            "Epoch 51/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.7865 - val_loss: 0.6465 - val_accuracy: 0.7078\n",
            "Epoch 52/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.7876 - val_loss: 0.6528 - val_accuracy: 0.6883\n",
            "Epoch 53/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4444 - accuracy: 0.7786 - val_loss: 0.6678 - val_accuracy: 0.6494\n",
            "Epoch 54/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.8122 - val_loss: 0.6428 - val_accuracy: 0.6948\n",
            "Epoch 55/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8040 - val_loss: 0.8200 - val_accuracy: 0.6494\n",
            "Epoch 56/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7542 - val_loss: 0.6277 - val_accuracy: 0.6883\n",
            "Epoch 57/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4708 - accuracy: 0.7560 - val_loss: 0.6373 - val_accuracy: 0.7013\n",
            "Epoch 58/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8317 - val_loss: 0.6338 - val_accuracy: 0.7273\n",
            "Epoch 59/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.7870 - val_loss: 0.6190 - val_accuracy: 0.7403\n",
            "Epoch 60/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4095 - accuracy: 0.8352 - val_loss: 0.6561 - val_accuracy: 0.6948\n",
            "Epoch 61/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.7789 - val_loss: 0.6766 - val_accuracy: 0.6753\n",
            "Epoch 62/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.7784 - val_loss: 0.7480 - val_accuracy: 0.6688\n",
            "Epoch 63/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.7738 - val_loss: 0.6912 - val_accuracy: 0.6883\n",
            "Epoch 64/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.7958 - val_loss: 0.6788 - val_accuracy: 0.6688\n",
            "Epoch 65/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8196 - val_loss: 0.7029 - val_accuracy: 0.6623\n",
            "Epoch 66/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8018 - val_loss: 0.7400 - val_accuracy: 0.6623\n",
            "Epoch 67/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8091 - val_loss: 0.6763 - val_accuracy: 0.7013\n",
            "Epoch 68/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.7923 - val_loss: 0.7318 - val_accuracy: 0.6883\n",
            "Epoch 69/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.7858 - val_loss: 0.6964 - val_accuracy: 0.6883\n",
            "Epoch 70/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.7916 - val_loss: 0.6716 - val_accuracy: 0.7143\n",
            "Epoch 71/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.8258 - val_loss: 0.7244 - val_accuracy: 0.6883\n",
            "Epoch 72/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4181 - accuracy: 0.8057 - val_loss: 0.7538 - val_accuracy: 0.6299\n",
            "Epoch 73/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.7972 - val_loss: 0.6830 - val_accuracy: 0.6688\n",
            "Epoch 74/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8013 - val_loss: 0.6936 - val_accuracy: 0.7078\n",
            "Epoch 75/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4151 - accuracy: 0.7947 - val_loss: 0.6407 - val_accuracy: 0.7338\n",
            "Epoch 76/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.7968 - val_loss: 0.7558 - val_accuracy: 0.7078\n",
            "Epoch 77/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8113 - val_loss: 0.6311 - val_accuracy: 0.7078\n",
            "Epoch 78/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8231 - val_loss: 0.6693 - val_accuracy: 0.7208\n",
            "Epoch 79/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4085 - accuracy: 0.7888 - val_loss: 0.7115 - val_accuracy: 0.6688\n",
            "Epoch 80/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3843 - accuracy: 0.8277 - val_loss: 0.7237 - val_accuracy: 0.6623\n",
            "Epoch 81/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8064 - val_loss: 0.7068 - val_accuracy: 0.7143\n",
            "Epoch 82/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8412 - val_loss: 0.6629 - val_accuracy: 0.7468\n",
            "Epoch 83/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3527 - accuracy: 0.8454 - val_loss: 0.7254 - val_accuracy: 0.6948\n",
            "Epoch 84/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8287 - val_loss: 0.6644 - val_accuracy: 0.6948\n",
            "Epoch 85/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8147 - val_loss: 0.7365 - val_accuracy: 0.7078\n",
            "Epoch 86/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8416 - val_loss: 0.7829 - val_accuracy: 0.7013\n",
            "Epoch 87/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.8265 - val_loss: 0.8020 - val_accuracy: 0.6883\n",
            "Epoch 88/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8583 - val_loss: 0.7216 - val_accuracy: 0.7273\n",
            "Epoch 89/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8242 - val_loss: 0.7747 - val_accuracy: 0.6753\n",
            "Epoch 90/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4138 - accuracy: 0.8241 - val_loss: 0.7817 - val_accuracy: 0.6818\n",
            "Epoch 91/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3434 - accuracy: 0.8541 - val_loss: 0.7650 - val_accuracy: 0.6818\n",
            "Epoch 92/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3596 - accuracy: 0.8374 - val_loss: 0.7286 - val_accuracy: 0.7208\n",
            "Epoch 93/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8568 - val_loss: 0.7429 - val_accuracy: 0.7273\n",
            "Epoch 94/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.8167 - val_loss: 0.7703 - val_accuracy: 0.6753\n",
            "Epoch 95/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4058 - accuracy: 0.8156 - val_loss: 0.6665 - val_accuracy: 0.7338\n",
            "Epoch 96/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8251 - val_loss: 0.7859 - val_accuracy: 0.7013\n",
            "Epoch 97/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.8254 - val_loss: 0.7796 - val_accuracy: 0.6753\n",
            "Epoch 98/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8321 - val_loss: 0.7774 - val_accuracy: 0.6688\n",
            "Epoch 99/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8592 - val_loss: 0.7593 - val_accuracy: 0.7403\n",
            "Epoch 100/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4118 - accuracy: 0.8100 - val_loss: 0.8073 - val_accuracy: 0.6948\n",
            "Epoch 101/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8430 - val_loss: 0.7077 - val_accuracy: 0.7078\n",
            "Epoch 102/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8482 - val_loss: 0.7268 - val_accuracy: 0.6948\n",
            "Epoch 103/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8546 - val_loss: 0.7652 - val_accuracy: 0.6688\n",
            "Epoch 104/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8559 - val_loss: 0.7490 - val_accuracy: 0.6688\n",
            "Epoch 105/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3808 - accuracy: 0.8197 - val_loss: 0.7616 - val_accuracy: 0.6753\n",
            "Epoch 106/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8342 - val_loss: 0.8990 - val_accuracy: 0.6753\n",
            "Epoch 107/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8277 - val_loss: 0.7664 - val_accuracy: 0.6948\n",
            "Epoch 108/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3515 - accuracy: 0.8279 - val_loss: 0.7946 - val_accuracy: 0.6883\n",
            "Epoch 109/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8525 - val_loss: 0.8044 - val_accuracy: 0.6753\n",
            "Epoch 110/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2979 - accuracy: 0.8794 - val_loss: 0.8529 - val_accuracy: 0.6558\n",
            "Epoch 111/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8452 - val_loss: 0.8744 - val_accuracy: 0.6558\n",
            "Epoch 112/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3163 - accuracy: 0.8636 - val_loss: 0.8140 - val_accuracy: 0.6753\n",
            "Epoch 113/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3341 - accuracy: 0.8478 - val_loss: 0.8489 - val_accuracy: 0.6948\n",
            "Epoch 114/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8704 - val_loss: 0.8236 - val_accuracy: 0.6688\n",
            "Epoch 115/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8511 - val_loss: 0.8335 - val_accuracy: 0.7078\n",
            "Epoch 116/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8388 - val_loss: 0.8078 - val_accuracy: 0.6948\n",
            "Epoch 117/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8463 - val_loss: 0.7972 - val_accuracy: 0.6818\n",
            "Epoch 118/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.8740 - val_loss: 0.7812 - val_accuracy: 0.7013\n",
            "Epoch 119/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.8745 - val_loss: 0.8406 - val_accuracy: 0.6948\n",
            "Epoch 120/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8741 - val_loss: 0.7744 - val_accuracy: 0.7013\n",
            "Epoch 121/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3198 - accuracy: 0.8580 - val_loss: 0.8184 - val_accuracy: 0.7143\n",
            "Epoch 122/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2926 - accuracy: 0.8879 - val_loss: 0.8981 - val_accuracy: 0.6753\n",
            "Epoch 123/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.8568 - val_loss: 0.8399 - val_accuracy: 0.6753\n",
            "Epoch 124/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.8746 - val_loss: 0.8517 - val_accuracy: 0.6818\n",
            "Epoch 125/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2711 - accuracy: 0.8935 - val_loss: 0.7841 - val_accuracy: 0.6753\n",
            "Epoch 126/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2996 - accuracy: 0.8709 - val_loss: 0.9375 - val_accuracy: 0.6948\n",
            "Epoch 127/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2751 - accuracy: 0.8684 - val_loss: 0.9173 - val_accuracy: 0.6753\n",
            "Epoch 128/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3159 - accuracy: 0.8622 - val_loss: 0.8717 - val_accuracy: 0.6818\n",
            "Epoch 129/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2908 - accuracy: 0.8668 - val_loss: 0.9036 - val_accuracy: 0.6494\n",
            "Epoch 130/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.8687 - val_loss: 0.8656 - val_accuracy: 0.6688\n",
            "Epoch 131/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8703 - val_loss: 0.9708 - val_accuracy: 0.6558\n",
            "Epoch 132/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2933 - accuracy: 0.8682 - val_loss: 0.9629 - val_accuracy: 0.7468\n",
            "Epoch 133/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3205 - accuracy: 0.8751 - val_loss: 0.8958 - val_accuracy: 0.6558\n",
            "Epoch 134/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.8884 - val_loss: 0.9935 - val_accuracy: 0.6753\n",
            "Epoch 135/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8502 - val_loss: 0.9519 - val_accuracy: 0.6623\n",
            "Epoch 136/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.8728 - val_loss: 0.9609 - val_accuracy: 0.6688\n",
            "Epoch 137/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.8609 - val_loss: 0.9957 - val_accuracy: 0.6818\n",
            "Epoch 138/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8715 - val_loss: 0.9119 - val_accuracy: 0.6753\n",
            "Epoch 139/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8773 - val_loss: 1.0249 - val_accuracy: 0.6623\n",
            "Epoch 140/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.8639 - val_loss: 0.9624 - val_accuracy: 0.6818\n",
            "Epoch 141/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.9081 - val_loss: 0.9121 - val_accuracy: 0.6558\n",
            "Epoch 142/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8722 - val_loss: 0.9842 - val_accuracy: 0.6818\n",
            "Epoch 143/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2943 - accuracy: 0.8606 - val_loss: 1.0246 - val_accuracy: 0.6883\n",
            "Epoch 144/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3053 - accuracy: 0.8746 - val_loss: 1.0000 - val_accuracy: 0.6688\n",
            "Epoch 145/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.8688 - val_loss: 0.9493 - val_accuracy: 0.7078\n",
            "Epoch 146/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2706 - accuracy: 0.8833 - val_loss: 0.9690 - val_accuracy: 0.6558\n",
            "Epoch 147/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2932 - accuracy: 0.8660 - val_loss: 1.0156 - val_accuracy: 0.6753\n",
            "Epoch 148/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9119 - val_loss: 0.9742 - val_accuracy: 0.6948\n",
            "Epoch 149/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.8878 - val_loss: 1.0671 - val_accuracy: 0.6753\n",
            "Epoch 150/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2992 - accuracy: 0.8785 - val_loss: 1.0133 - val_accuracy: 0.6494\n",
            "Epoch 151/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2721 - accuracy: 0.8911 - val_loss: 0.9921 - val_accuracy: 0.6623\n",
            "Epoch 152/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2350 - accuracy: 0.9062 - val_loss: 1.0383 - val_accuracy: 0.6494\n",
            "Epoch 153/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.9109 - val_loss: 1.0958 - val_accuracy: 0.6558\n",
            "Epoch 154/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.9019 - val_loss: 1.0939 - val_accuracy: 0.6623\n",
            "Epoch 155/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.8782 - val_loss: 1.1231 - val_accuracy: 0.6818\n",
            "Epoch 156/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.9092 - val_loss: 1.1228 - val_accuracy: 0.6299\n",
            "Epoch 157/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9048 - val_loss: 1.0210 - val_accuracy: 0.6818\n",
            "Epoch 158/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8772 - val_loss: 1.0161 - val_accuracy: 0.6818\n",
            "Epoch 159/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8594 - val_loss: 1.0187 - val_accuracy: 0.6753\n",
            "Epoch 160/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.8897 - val_loss: 1.0397 - val_accuracy: 0.6948\n",
            "Epoch 161/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9037 - val_loss: 1.0605 - val_accuracy: 0.6429\n",
            "Epoch 162/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2350 - accuracy: 0.8981 - val_loss: 1.0255 - val_accuracy: 0.6623\n",
            "Epoch 163/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.8863 - val_loss: 1.1703 - val_accuracy: 0.6753\n",
            "Epoch 164/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.9030 - val_loss: 1.1576 - val_accuracy: 0.6623\n",
            "Epoch 165/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.9086 - val_loss: 1.2280 - val_accuracy: 0.6234\n",
            "Epoch 166/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2315 - accuracy: 0.9060 - val_loss: 1.0982 - val_accuracy: 0.6429\n",
            "Epoch 167/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.9008 - val_loss: 1.2241 - val_accuracy: 0.6753\n",
            "Epoch 168/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2212 - accuracy: 0.9092 - val_loss: 1.1565 - val_accuracy: 0.6234\n",
            "Epoch 169/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2556 - accuracy: 0.8922 - val_loss: 1.2166 - val_accuracy: 0.6364\n",
            "Epoch 170/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2598 - accuracy: 0.8742 - val_loss: 1.2154 - val_accuracy: 0.6558\n",
            "Epoch 171/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2401 - accuracy: 0.9073 - val_loss: 1.0258 - val_accuracy: 0.6623\n",
            "Epoch 172/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9128 - val_loss: 1.1331 - val_accuracy: 0.6623\n",
            "Epoch 173/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2158 - accuracy: 0.8974 - val_loss: 1.0409 - val_accuracy: 0.7013\n",
            "Epoch 174/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.9286 - val_loss: 1.0781 - val_accuracy: 0.6948\n",
            "Epoch 175/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2094 - accuracy: 0.9110 - val_loss: 1.1310 - val_accuracy: 0.6753\n",
            "Epoch 176/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9012 - val_loss: 1.1782 - val_accuracy: 0.6623\n",
            "Epoch 177/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.9369 - val_loss: 1.0996 - val_accuracy: 0.6623\n",
            "Epoch 178/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9235 - val_loss: 1.1963 - val_accuracy: 0.6429\n",
            "Epoch 179/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.9266 - val_loss: 1.1577 - val_accuracy: 0.6558\n",
            "Epoch 180/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1885 - accuracy: 0.9138 - val_loss: 1.2842 - val_accuracy: 0.6688\n",
            "Epoch 181/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2060 - accuracy: 0.9159 - val_loss: 1.2136 - val_accuracy: 0.6623\n",
            "Epoch 182/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1981 - accuracy: 0.9197 - val_loss: 1.1392 - val_accuracy: 0.6753\n",
            "Epoch 183/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2501 - accuracy: 0.8770 - val_loss: 1.3107 - val_accuracy: 0.6558\n",
            "Epoch 184/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1819 - accuracy: 0.9301 - val_loss: 1.2756 - val_accuracy: 0.6753\n",
            "Epoch 185/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.9276 - val_loss: 1.1679 - val_accuracy: 0.6623\n",
            "Epoch 186/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9073 - val_loss: 1.3080 - val_accuracy: 0.6364\n",
            "Epoch 187/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1720 - accuracy: 0.9426 - val_loss: 1.2276 - val_accuracy: 0.6558\n",
            "Epoch 188/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2056 - accuracy: 0.9104 - val_loss: 1.2618 - val_accuracy: 0.6299\n",
            "Epoch 189/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.8862 - val_loss: 1.2383 - val_accuracy: 0.6753\n",
            "Epoch 190/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9191 - val_loss: 1.2908 - val_accuracy: 0.6494\n",
            "Epoch 191/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2125 - accuracy: 0.9231 - val_loss: 1.1739 - val_accuracy: 0.6818\n",
            "Epoch 192/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.9334 - val_loss: 1.4147 - val_accuracy: 0.6623\n",
            "Epoch 193/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1661 - accuracy: 0.9429 - val_loss: 1.4312 - val_accuracy: 0.6364\n",
            "Epoch 194/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9073 - val_loss: 1.4643 - val_accuracy: 0.6494\n",
            "Epoch 195/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1652 - accuracy: 0.9504 - val_loss: 1.4544 - val_accuracy: 0.6558\n",
            "Epoch 196/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9209 - val_loss: 1.4815 - val_accuracy: 0.6299\n",
            "Epoch 197/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1650 - accuracy: 0.9435 - val_loss: 1.4239 - val_accuracy: 0.6429\n",
            "Epoch 198/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1841 - accuracy: 0.9336 - val_loss: 1.4849 - val_accuracy: 0.6299\n",
            "Epoch 199/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1517 - accuracy: 0.9383 - val_loss: 1.5331 - val_accuracy: 0.6818\n",
            "Epoch 200/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1763 - accuracy: 0.9251 - val_loss: 1.4369 - val_accuracy: 0.6883\n",
            "Epoch 201/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.9069 - val_loss: 1.4938 - val_accuracy: 0.6429\n",
            "Epoch 202/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1837 - accuracy: 0.9393 - val_loss: 1.3915 - val_accuracy: 0.6494\n",
            "Epoch 203/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1821 - accuracy: 0.9315 - val_loss: 1.2953 - val_accuracy: 0.6429\n",
            "Epoch 204/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9443 - val_loss: 1.3661 - val_accuracy: 0.6623\n",
            "Epoch 205/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1688 - accuracy: 0.9438 - val_loss: 1.3669 - val_accuracy: 0.6818\n",
            "Epoch 206/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1683 - accuracy: 0.9410 - val_loss: 1.5326 - val_accuracy: 0.6429\n",
            "Epoch 207/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1824 - accuracy: 0.9376 - val_loss: 1.4958 - val_accuracy: 0.6429\n",
            "Epoch 208/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1843 - accuracy: 0.9161 - val_loss: 1.4263 - val_accuracy: 0.6558\n",
            "Epoch 209/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2226 - accuracy: 0.8959 - val_loss: 1.5174 - val_accuracy: 0.6364\n",
            "Epoch 210/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1484 - accuracy: 0.9467 - val_loss: 1.5419 - val_accuracy: 0.6429\n",
            "Epoch 211/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9550 - val_loss: 1.4349 - val_accuracy: 0.6558\n",
            "Epoch 212/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1749 - accuracy: 0.9355 - val_loss: 1.5204 - val_accuracy: 0.6688\n",
            "Epoch 213/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9311 - val_loss: 1.4841 - val_accuracy: 0.6753\n",
            "Epoch 214/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1586 - accuracy: 0.9331 - val_loss: 1.4589 - val_accuracy: 0.6364\n",
            "Epoch 215/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9668 - val_loss: 1.5639 - val_accuracy: 0.6558\n",
            "Epoch 216/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1488 - accuracy: 0.9398 - val_loss: 1.5600 - val_accuracy: 0.6299\n",
            "Epoch 217/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2096 - accuracy: 0.9179 - val_loss: 1.5988 - val_accuracy: 0.6688\n",
            "Epoch 218/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9321 - val_loss: 1.5103 - val_accuracy: 0.6753\n",
            "Epoch 219/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1666 - accuracy: 0.9455 - val_loss: 1.6076 - val_accuracy: 0.6494\n",
            "Epoch 220/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9520 - val_loss: 1.5843 - val_accuracy: 0.6104\n",
            "Epoch 221/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1400 - accuracy: 0.9559 - val_loss: 1.6762 - val_accuracy: 0.6104\n",
            "Epoch 222/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9518 - val_loss: 1.5549 - val_accuracy: 0.6558\n",
            "Epoch 223/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9599 - val_loss: 1.4504 - val_accuracy: 0.6883\n",
            "Epoch 224/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1283 - accuracy: 0.9500 - val_loss: 1.6490 - val_accuracy: 0.6558\n",
            "Epoch 225/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9485 - val_loss: 1.4676 - val_accuracy: 0.6623\n",
            "Epoch 226/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9259 - val_loss: 1.4352 - val_accuracy: 0.6623\n",
            "Epoch 227/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.9448 - val_loss: 1.6725 - val_accuracy: 0.6753\n",
            "Epoch 228/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1564 - accuracy: 0.9296 - val_loss: 1.4184 - val_accuracy: 0.6558\n",
            "Epoch 229/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1883 - accuracy: 0.9296 - val_loss: 1.5275 - val_accuracy: 0.6494\n",
            "Epoch 230/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.9038 - val_loss: 1.6916 - val_accuracy: 0.6494\n",
            "Epoch 231/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.9082 - val_loss: 1.6062 - val_accuracy: 0.6429\n",
            "Epoch 232/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1476 - accuracy: 0.9552 - val_loss: 1.5244 - val_accuracy: 0.6494\n",
            "Epoch 233/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1589 - accuracy: 0.9443 - val_loss: 1.7377 - val_accuracy: 0.6494\n",
            "Epoch 234/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1956 - accuracy: 0.9100 - val_loss: 1.5837 - val_accuracy: 0.6753\n",
            "Epoch 235/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1732 - accuracy: 0.9288 - val_loss: 1.5960 - val_accuracy: 0.6623\n",
            "Epoch 236/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.9409 - val_loss: 1.7867 - val_accuracy: 0.6558\n",
            "Epoch 237/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9338 - val_loss: 1.5595 - val_accuracy: 0.6753\n",
            "Epoch 238/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1421 - accuracy: 0.9477 - val_loss: 1.6698 - val_accuracy: 0.6494\n",
            "Epoch 239/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1173 - accuracy: 0.9591 - val_loss: 1.7373 - val_accuracy: 0.6299\n",
            "Epoch 240/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0955 - accuracy: 0.9666 - val_loss: 1.7580 - val_accuracy: 0.6364\n",
            "Epoch 241/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1353 - accuracy: 0.9469 - val_loss: 1.7433 - val_accuracy: 0.6494\n",
            "Epoch 242/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1554 - accuracy: 0.9332 - val_loss: 1.7546 - val_accuracy: 0.6429\n",
            "Epoch 243/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.9482 - val_loss: 1.8451 - val_accuracy: 0.6494\n",
            "Epoch 244/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0918 - accuracy: 0.9772 - val_loss: 1.6868 - val_accuracy: 0.6558\n",
            "Epoch 245/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1211 - accuracy: 0.9604 - val_loss: 1.7571 - val_accuracy: 0.6299\n",
            "Epoch 246/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9779 - val_loss: 1.8111 - val_accuracy: 0.6494\n",
            "Epoch 247/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9860 - val_loss: 2.0029 - val_accuracy: 0.6429\n",
            "Epoch 248/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1073 - accuracy: 0.9606 - val_loss: 1.8227 - val_accuracy: 0.6429\n",
            "Epoch 249/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9763 - val_loss: 1.8069 - val_accuracy: 0.6688\n",
            "Epoch 250/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1144 - accuracy: 0.9565 - val_loss: 1.8769 - val_accuracy: 0.6558\n",
            "Epoch 251/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9388 - val_loss: 1.9191 - val_accuracy: 0.6558\n",
            "Epoch 252/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.9476 - val_loss: 1.8642 - val_accuracy: 0.6623\n",
            "Epoch 253/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.9287 - val_loss: 2.2192 - val_accuracy: 0.6169\n",
            "Epoch 254/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2005 - accuracy: 0.9263 - val_loss: 2.0297 - val_accuracy: 0.6688\n",
            "Epoch 255/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8718 - val_loss: 1.6137 - val_accuracy: 0.6494\n",
            "Epoch 256/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.9234 - val_loss: 1.7378 - val_accuracy: 0.6494\n",
            "Epoch 257/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.9515 - val_loss: 1.7240 - val_accuracy: 0.6558\n",
            "Epoch 258/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.9686 - val_loss: 1.8686 - val_accuracy: 0.6558\n",
            "Epoch 259/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9801 - val_loss: 1.8349 - val_accuracy: 0.6688\n",
            "Epoch 260/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.9782 - val_loss: 1.7609 - val_accuracy: 0.6688\n",
            "Epoch 261/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0948 - accuracy: 0.9707 - val_loss: 1.9499 - val_accuracy: 0.6169\n",
            "Epoch 262/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9744 - val_loss: 1.8664 - val_accuracy: 0.6623\n",
            "Epoch 263/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9805 - val_loss: 1.8973 - val_accuracy: 0.6429\n",
            "Epoch 264/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1266 - accuracy: 0.9422 - val_loss: 1.9536 - val_accuracy: 0.6234\n",
            "Epoch 265/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1693 - accuracy: 0.9412 - val_loss: 1.9030 - val_accuracy: 0.6688\n",
            "Epoch 266/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9220 - val_loss: 1.8549 - val_accuracy: 0.6299\n",
            "Epoch 267/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9117 - val_loss: 1.7375 - val_accuracy: 0.6753\n",
            "Epoch 268/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9502 - val_loss: 1.7338 - val_accuracy: 0.6558\n",
            "Epoch 269/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9752 - val_loss: 1.9160 - val_accuracy: 0.6688\n",
            "Epoch 270/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.9635 - val_loss: 1.9580 - val_accuracy: 0.6429\n",
            "Epoch 271/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0946 - accuracy: 0.9667 - val_loss: 1.8565 - val_accuracy: 0.6364\n",
            "Epoch 272/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9552 - val_loss: 2.0634 - val_accuracy: 0.6364\n",
            "Epoch 273/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 0.9608 - val_loss: 2.0521 - val_accuracy: 0.6299\n",
            "Epoch 274/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9756 - val_loss: 1.9240 - val_accuracy: 0.6688\n",
            "Epoch 275/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.9782 - val_loss: 2.0358 - val_accuracy: 0.6623\n",
            "Epoch 276/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9836 - val_loss: 2.0663 - val_accuracy: 0.6494\n",
            "Epoch 277/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9884 - val_loss: 2.0194 - val_accuracy: 0.6364\n",
            "Epoch 278/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9753 - val_loss: 2.0240 - val_accuracy: 0.6623\n",
            "Epoch 279/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0891 - accuracy: 0.9798 - val_loss: 1.9397 - val_accuracy: 0.6623\n",
            "Epoch 280/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0912 - accuracy: 0.9687 - val_loss: 2.2068 - val_accuracy: 0.6169\n",
            "Epoch 281/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1516 - accuracy: 0.9370 - val_loss: 2.0307 - val_accuracy: 0.6169\n",
            "Epoch 282/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9175 - val_loss: 1.7827 - val_accuracy: 0.6623\n",
            "Epoch 283/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1296 - accuracy: 0.9484 - val_loss: 2.0706 - val_accuracy: 0.6234\n",
            "Epoch 284/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1981 - accuracy: 0.9338 - val_loss: 1.8190 - val_accuracy: 0.6429\n",
            "Epoch 285/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.9583 - val_loss: 1.9818 - val_accuracy: 0.6234\n",
            "Epoch 286/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1199 - accuracy: 0.9535 - val_loss: 2.0264 - val_accuracy: 0.6558\n",
            "Epoch 287/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1088 - accuracy: 0.9659 - val_loss: 2.0333 - val_accuracy: 0.6234\n",
            "Epoch 288/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1598 - accuracy: 0.9501 - val_loss: 1.7986 - val_accuracy: 0.6623\n",
            "Epoch 289/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0829 - accuracy: 0.9645 - val_loss: 2.1031 - val_accuracy: 0.6623\n",
            "Epoch 290/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9820 - val_loss: 2.0511 - val_accuracy: 0.6429\n",
            "Epoch 291/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9837 - val_loss: 2.1230 - val_accuracy: 0.6558\n",
            "Epoch 292/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9726 - val_loss: 2.1105 - val_accuracy: 0.6299\n",
            "Epoch 293/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9729 - val_loss: 2.2655 - val_accuracy: 0.6234\n",
            "Epoch 294/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0773 - accuracy: 0.9724 - val_loss: 2.3168 - val_accuracy: 0.6234\n",
            "Epoch 295/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.8788 - val_loss: 1.9282 - val_accuracy: 0.6494\n",
            "Epoch 296/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2104 - accuracy: 0.9096 - val_loss: 1.8703 - val_accuracy: 0.6753\n",
            "Epoch 297/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3606 - accuracy: 0.8761 - val_loss: 1.7983 - val_accuracy: 0.6234\n",
            "Epoch 298/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1701 - accuracy: 0.9478 - val_loss: 2.0460 - val_accuracy: 0.6494\n",
            "Epoch 299/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0892 - accuracy: 0.9776 - val_loss: 1.9764 - val_accuracy: 0.6429\n",
            "Epoch 300/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9669 - val_loss: 2.0063 - val_accuracy: 0.6494\n",
            "Epoch 301/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9615 - val_loss: 2.0175 - val_accuracy: 0.6429\n",
            "Epoch 302/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0877 - accuracy: 0.9636 - val_loss: 2.1256 - val_accuracy: 0.6299\n",
            "Epoch 303/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.9699 - val_loss: 2.1514 - val_accuracy: 0.6494\n",
            "Epoch 304/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1060 - accuracy: 0.9606 - val_loss: 2.1056 - val_accuracy: 0.6234\n",
            "Epoch 305/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9800 - val_loss: 2.1931 - val_accuracy: 0.6429\n",
            "Epoch 306/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9759 - val_loss: 2.1358 - val_accuracy: 0.6558\n",
            "Epoch 307/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.9742 - val_loss: 2.2245 - val_accuracy: 0.6364\n",
            "Epoch 308/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9766 - val_loss: 2.2225 - val_accuracy: 0.6364\n",
            "Epoch 309/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9724 - val_loss: 2.2876 - val_accuracy: 0.6429\n",
            "Epoch 310/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1000 - accuracy: 0.9622 - val_loss: 2.1793 - val_accuracy: 0.6104\n",
            "Epoch 311/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9625 - val_loss: 1.9441 - val_accuracy: 0.6104\n",
            "Epoch 312/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1739 - accuracy: 0.9421 - val_loss: 1.9659 - val_accuracy: 0.6169\n",
            "Epoch 313/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2102 - accuracy: 0.9098 - val_loss: 1.8495 - val_accuracy: 0.6364\n",
            "Epoch 314/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1371 - accuracy: 0.9392 - val_loss: 1.8274 - val_accuracy: 0.6494\n",
            "Epoch 315/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0923 - accuracy: 0.9761 - val_loss: 2.0543 - val_accuracy: 0.6364\n",
            "Epoch 316/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9854 - val_loss: 1.9663 - val_accuracy: 0.6299\n",
            "Epoch 317/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9871 - val_loss: 1.9912 - val_accuracy: 0.6169\n",
            "Epoch 318/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9879 - val_loss: 2.0482 - val_accuracy: 0.6364\n",
            "Epoch 319/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9896 - val_loss: 2.1160 - val_accuracy: 0.6494\n",
            "Epoch 320/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9794 - val_loss: 2.1956 - val_accuracy: 0.6299\n",
            "Epoch 321/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9831 - val_loss: 2.1150 - val_accuracy: 0.6364\n",
            "Epoch 322/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9834 - val_loss: 2.2143 - val_accuracy: 0.6299\n",
            "Epoch 323/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0682 - accuracy: 0.9768 - val_loss: 2.2158 - val_accuracy: 0.6429\n",
            "Epoch 324/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9786 - val_loss: 2.3079 - val_accuracy: 0.6494\n",
            "Epoch 325/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.9656 - val_loss: 2.2854 - val_accuracy: 0.6623\n",
            "Epoch 326/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9728 - val_loss: 2.2170 - val_accuracy: 0.6494\n",
            "Epoch 327/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9776 - val_loss: 2.3052 - val_accuracy: 0.6429\n",
            "Epoch 328/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0864 - accuracy: 0.9686 - val_loss: 2.2289 - val_accuracy: 0.6364\n",
            "Epoch 329/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0773 - accuracy: 0.9736 - val_loss: 2.0957 - val_accuracy: 0.6364\n",
            "Epoch 330/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1465 - accuracy: 0.9230 - val_loss: 2.0607 - val_accuracy: 0.6494\n",
            "Epoch 331/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8895 - val_loss: 1.9944 - val_accuracy: 0.6558\n",
            "Epoch 332/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9362 - val_loss: 2.2310 - val_accuracy: 0.6494\n",
            "Epoch 333/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9516 - val_loss: 2.2853 - val_accuracy: 0.6039\n",
            "Epoch 334/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1936 - accuracy: 0.9258 - val_loss: 1.9397 - val_accuracy: 0.6429\n",
            "Epoch 335/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1272 - accuracy: 0.9393 - val_loss: 2.1358 - val_accuracy: 0.6364\n",
            "Epoch 336/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.9352 - val_loss: 2.1953 - val_accuracy: 0.6039\n",
            "Epoch 337/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0868 - accuracy: 0.9810 - val_loss: 2.2609 - val_accuracy: 0.6558\n",
            "Epoch 338/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 0.9760 - val_loss: 2.3892 - val_accuracy: 0.6104\n",
            "Epoch 339/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0612 - accuracy: 0.9795 - val_loss: 2.2836 - val_accuracy: 0.6364\n",
            "Epoch 340/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9841 - val_loss: 2.3888 - val_accuracy: 0.6364\n",
            "Epoch 341/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9853 - val_loss: 2.4160 - val_accuracy: 0.6429\n",
            "Epoch 342/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9897 - val_loss: 2.2710 - val_accuracy: 0.6558\n",
            "Epoch 343/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9732 - val_loss: 2.2477 - val_accuracy: 0.6234\n",
            "Epoch 344/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0411 - accuracy: 0.9865 - val_loss: 2.2357 - val_accuracy: 0.6364\n",
            "Epoch 345/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9791 - val_loss: 2.3593 - val_accuracy: 0.6299\n",
            "Epoch 346/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.9912 - val_loss: 2.4785 - val_accuracy: 0.6494\n",
            "Epoch 347/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9673 - val_loss: 2.3817 - val_accuracy: 0.6234\n",
            "Epoch 348/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9735 - val_loss: 2.3902 - val_accuracy: 0.6494\n",
            "Epoch 349/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9799 - val_loss: 2.4700 - val_accuracy: 0.6234\n",
            "Epoch 350/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9775 - val_loss: 2.4977 - val_accuracy: 0.6299\n",
            "Epoch 351/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.9793 - val_loss: 2.5836 - val_accuracy: 0.6688\n",
            "Epoch 352/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1520 - accuracy: 0.9551 - val_loss: 2.4970 - val_accuracy: 0.6364\n",
            "Epoch 353/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0825 - accuracy: 0.9769 - val_loss: 2.3340 - val_accuracy: 0.6299\n",
            "Epoch 354/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9770 - val_loss: 2.5417 - val_accuracy: 0.6039\n",
            "Epoch 355/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9875 - val_loss: 2.5307 - val_accuracy: 0.5974\n",
            "Epoch 356/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9692 - val_loss: 2.5082 - val_accuracy: 0.6429\n",
            "Epoch 357/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9712 - val_loss: 2.4990 - val_accuracy: 0.6104\n",
            "Epoch 358/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9718 - val_loss: 2.6023 - val_accuracy: 0.6299\n",
            "Epoch 359/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1011 - accuracy: 0.9649 - val_loss: 2.5313 - val_accuracy: 0.6169\n",
            "Epoch 360/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1728 - accuracy: 0.9256 - val_loss: 2.9961 - val_accuracy: 0.6623\n",
            "Epoch 361/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4240 - accuracy: 0.8847 - val_loss: 2.3957 - val_accuracy: 0.6623\n",
            "Epoch 362/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.9300 - val_loss: 2.3810 - val_accuracy: 0.6623\n",
            "Epoch 363/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0702 - accuracy: 0.9782 - val_loss: 2.3052 - val_accuracy: 0.6429\n",
            "Epoch 364/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9831 - val_loss: 2.3843 - val_accuracy: 0.6039\n",
            "Epoch 365/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9793 - val_loss: 2.3120 - val_accuracy: 0.6623\n",
            "Epoch 366/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9726 - val_loss: 2.3701 - val_accuracy: 0.6364\n",
            "Epoch 367/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9655 - val_loss: 2.2983 - val_accuracy: 0.6623\n",
            "Epoch 368/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9831 - val_loss: 2.3499 - val_accuracy: 0.6558\n",
            "Epoch 369/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9799 - val_loss: 2.3133 - val_accuracy: 0.6429\n",
            "Epoch 370/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.9864 - val_loss: 2.4877 - val_accuracy: 0.6364\n",
            "Epoch 371/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9830 - val_loss: 2.5352 - val_accuracy: 0.6429\n",
            "Epoch 372/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0394 - accuracy: 0.9931 - val_loss: 2.7940 - val_accuracy: 0.6169\n",
            "Epoch 373/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9825 - val_loss: 2.5641 - val_accuracy: 0.6429\n",
            "Epoch 374/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.9976 - val_loss: 2.5363 - val_accuracy: 0.6623\n",
            "Epoch 375/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9847 - val_loss: 2.6198 - val_accuracy: 0.6623\n",
            "Epoch 376/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1102 - accuracy: 0.9476 - val_loss: 2.3741 - val_accuracy: 0.6429\n",
            "Epoch 377/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1590 - accuracy: 0.9268 - val_loss: 2.7103 - val_accuracy: 0.6623\n",
            "Epoch 378/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9294 - val_loss: 2.5590 - val_accuracy: 0.6169\n",
            "Epoch 379/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9277 - val_loss: 2.7203 - val_accuracy: 0.6429\n",
            "Epoch 380/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.9700 - val_loss: 2.7339 - val_accuracy: 0.6494\n",
            "Epoch 381/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1562 - accuracy: 0.9626 - val_loss: 2.4477 - val_accuracy: 0.6169\n",
            "Epoch 382/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9585 - val_loss: 2.5634 - val_accuracy: 0.6688\n",
            "Epoch 383/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9677 - val_loss: 2.5095 - val_accuracy: 0.6558\n",
            "Epoch 384/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0781 - accuracy: 0.9704 - val_loss: 2.6692 - val_accuracy: 0.6429\n",
            "Epoch 385/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.9865 - val_loss: 2.6201 - val_accuracy: 0.6494\n",
            "Epoch 386/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0394 - accuracy: 0.9913 - val_loss: 2.7466 - val_accuracy: 0.6429\n",
            "Epoch 387/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 0.9966 - val_loss: 2.7171 - val_accuracy: 0.6558\n",
            "Epoch 388/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.9930 - val_loss: 2.6090 - val_accuracy: 0.6623\n",
            "Epoch 389/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0822 - accuracy: 0.9728 - val_loss: 2.8066 - val_accuracy: 0.6558\n",
            "Epoch 390/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9751 - val_loss: 2.7990 - val_accuracy: 0.6429\n",
            "Epoch 391/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.9916 - val_loss: 2.8271 - val_accuracy: 0.6169\n",
            "Epoch 392/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.9972 - val_loss: 2.7424 - val_accuracy: 0.6364\n",
            "Epoch 393/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9824 - val_loss: 2.8052 - val_accuracy: 0.6494\n",
            "Epoch 394/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9903 - val_loss: 2.7785 - val_accuracy: 0.6558\n",
            "Epoch 395/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0672 - accuracy: 0.9825 - val_loss: 2.8929 - val_accuracy: 0.6299\n",
            "Epoch 396/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1181 - accuracy: 0.9614 - val_loss: 2.7220 - val_accuracy: 0.6364\n",
            "Epoch 397/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1555 - accuracy: 0.9366 - val_loss: 2.7745 - val_accuracy: 0.6104\n",
            "Epoch 398/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1376 - accuracy: 0.9622 - val_loss: 2.5853 - val_accuracy: 0.6688\n",
            "Epoch 399/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1382 - accuracy: 0.9294 - val_loss: 2.4728 - val_accuracy: 0.6623\n",
            "Epoch 400/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9743 - val_loss: 2.6621 - val_accuracy: 0.6364\n",
            "Epoch 401/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0384 - accuracy: 0.9908 - val_loss: 2.7858 - val_accuracy: 0.6299\n",
            "Epoch 402/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 0.9825 - val_loss: 2.6721 - val_accuracy: 0.6558\n",
            "Epoch 403/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.9925 - val_loss: 2.8688 - val_accuracy: 0.6299\n",
            "Epoch 404/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.9921 - val_loss: 2.6325 - val_accuracy: 0.6364\n",
            "Epoch 405/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 0.9934 - val_loss: 2.8300 - val_accuracy: 0.6299\n",
            "Epoch 406/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0321 - accuracy: 0.9961 - val_loss: 2.6485 - val_accuracy: 0.6558\n",
            "Epoch 407/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0319 - accuracy: 0.9930 - val_loss: 2.8015 - val_accuracy: 0.6364\n",
            "Epoch 408/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9658 - val_loss: 2.8288 - val_accuracy: 0.6429\n",
            "Epoch 409/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9795 - val_loss: 2.5798 - val_accuracy: 0.6364\n",
            "Epoch 410/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9838 - val_loss: 2.6815 - val_accuracy: 0.6494\n",
            "Epoch 411/1000\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0429 - accuracy: 0.9858 - val_loss: 2.6400 - val_accuracy: 0.6299\n",
            "Epoch 412/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.9967 - val_loss: 2.7555 - val_accuracy: 0.6558\n",
            "Epoch 413/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 0.9891 - val_loss: 2.8087 - val_accuracy: 0.6364\n",
            "Epoch 414/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9844 - val_loss: 2.8090 - val_accuracy: 0.6364\n",
            "Epoch 415/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.9834 - val_loss: 2.8959 - val_accuracy: 0.6299\n",
            "Epoch 416/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9877 - val_loss: 2.9981 - val_accuracy: 0.6234\n",
            "Epoch 417/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9952 - val_loss: 2.9097 - val_accuracy: 0.6299\n",
            "Epoch 418/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9910 - val_loss: 2.9588 - val_accuracy: 0.6104\n",
            "Epoch 419/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9916 - val_loss: 3.0849 - val_accuracy: 0.6494\n",
            "Epoch 420/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.9903 - val_loss: 3.0470 - val_accuracy: 0.6364\n",
            "Epoch 421/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9888 - val_loss: 3.0990 - val_accuracy: 0.6364\n",
            "Epoch 422/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9764 - val_loss: 2.8070 - val_accuracy: 0.6494\n",
            "Epoch 423/1000\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9855 - val_loss: 3.0293 - val_accuracy: 0.6429\n",
            "Epoch 424/1000\n",
            "42/62 [===================>..........] - ETA: 0s - loss: 0.2529 - accuracy: 0.9076"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXE_uDmCj8jz"
      },
      "source": [
        "Let's test a couple of sample in the test set to see what is given"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34XZbMn_j8j0"
      },
      "source": [
        "sample1 = np.array([x_test[0]])\n",
        "sample2 = np.array([x_test[1]])\n",
        "sample1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7nyb53qj8j0"
      },
      "source": [
        "result = model.predict_classes(sample1)\n",
        "\n",
        "if result==0:\n",
        "    print(\"NO Diabetes\")\n",
        "else:\n",
        "    print(\"Diabetes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSjHiiB1j8j0"
      },
      "source": [
        "### Exercise 3\n",
        "Try stuff!\n",
        "\n",
        "Add more layers, change the number of neurons in each layer (there doesn't have to be the same amount in each layer), change the optimiser to `'sgd'`, see what the highest accuracy you can get is. This is also stuff you can try in the iterative development mission of your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjaoeDigj8j0"
      },
      "source": [
        "#### Overfitting\n",
        "Compare the training accuracy and the testing accuracy (it is called val_accuracy above). The training accuracy is much higher. This is perfectly normal, but also a good indication that the model is memorising the data. Recall from the lecture that this is called overfitting.\n",
        "\n",
        "Overfitting can be tackled by a technique called dropout, where a proportion, $p$, of the nodes within each layer of the neural network are randomly eliminated. \n",
        "We will try $p=0.3$, which eliminates 30% of the nodes.\n",
        "This stops layers of hidden neurons being overly reliant on a small number of nodes, which can often happen when the data set is small and can be easily estimated.\n",
        "\n",
        "To add a dropout layer, we simply `model.add(Dropout(0.3))`. In this instance we can define a new model called `model2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIoV0YhAj8j1"
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Dense(50, input_dim=8, activation='relu'))\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(Dense(50, activation='relu'))\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(Dense(50, activation='relu'))\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.fit(x_train,y_train, epochs=1000, batch_size=70, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS7osadRj8j1"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "We've improved the test accuracy but the training accuracy has massively decreased. This might be because too many neurons have been dropped out and we are not longer learning effectively. Try changing the dropout parameter to see what gives the best results."
      ]
    }
  ]
}